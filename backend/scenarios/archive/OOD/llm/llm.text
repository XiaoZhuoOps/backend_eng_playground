问题一：提升 Agent 离线处理文档的吞吐量
这个场景是，用户在前台提交请求，后台由 Agent 慢慢处理生成文档，生成完毕之后通知用户。每个任务大概要十几分钟时间，中间分为多个阶段，涉及到不同大小模型的混合使用。

一开始的思路是，既然是离线处理，就是用消息队列解耦。用户提交请求后，加入到消息队列。离线的 worker 从队列中读取任务。任务完成之后再写入消息队列，修改状态，通知用户。

架构大致如下


尝试一：增加消费实例数
但是实测下来吞吐量很低，尝试增加 Kafka 消费的实例数（阿里云的 Kafka 默认 12 个分区，实例数要小于分区数）。

但是又会遇到新的问题：大模型接口有 Tokens Per Minute 上限，不能无限增加实例数。

尝试二：增加限流器，约束同时请求模型的并发
在消费者实例间共享漏桶限流器，请求模型接口前判断当前分钟是否达到阈值（减去预留）。否则等待到限流器放行。
但是这样没有解决问题，整个系统的吞吐量太小。查看监控后发现，大部分情况下都跑不满阈值。

这里 agent 场景会调用工具，和 llm 多轮对话。我的设计中是只在调用 llm 时限制，所以就会有 worker 被阻塞，或者因为调用工具等原因，调用模型的占空比比较低。